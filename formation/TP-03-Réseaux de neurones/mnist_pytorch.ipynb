{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST en Deep Learning\n",
    "\n",
    "Dans ce TP, nous allons construire des algorithmes de Deep Learning pour tenter de reconnaître des chiffres manuscrits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données et transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons travailler sur la base de données MNIST qui contient 60000 images en niveaux de grille de résolution 28x28, représentant les 10 chiffres de 0 à 9, ainsi qu'un jeu de test de 10000 images. Tout d'abord, chargeons ce jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Les expériences ci dessous seront lancées sur {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Téléchargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    \"../data\", train=True, transform=ToTensor(), download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"../data\", train=False, transform=ToTensor(), download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un dataloader et visualiser des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "\n",
    "# Parcourez un batch dans le DataLoader\n",
    "for images, labels in train_dataloader:\n",
    "    print(\n",
    "        \"BATCH INFO : input shape: \",\n",
    "        images.shape,\n",
    "        \" , labels shape: \",\n",
    "        labels.shape,\n",
    "        \"\\n\",\n",
    "    )\n",
    "    sample_images = images[:5]  # Les 5 premières images\n",
    "    sample_labels = labels[:5]  # Les 5 premières étiquettes\n",
    "    break  # On prend seulement le premier batch\n",
    "\n",
    "# Affichez les images et leurs étiquettes\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(\n",
    "        sample_images[i].squeeze(), cmap=\"gray\"\n",
    "    )  # .squeeze() pour retirer la dimension du canal\n",
    "    ax.set_title(f\"Label: {sample_labels[i].item()}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire un premier réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construisons notre premier réseau de neurones.\n",
    "\n",
    "Pour cela, nous allons créer un modèle Pytorch en utilisant la classe `nn.Module` vu précedemment.\n",
    "\n",
    "Puis utiliser les méthodes suivantes de Pytroch pour ajouter des couches à ce modèle :\n",
    "\n",
    "* `nn.FLatten` : on manipule des vecteurs et non des image, on passe de (28,28) -> (784,)\n",
    "* `nn.Linear` : on ajoute une couche dense (ou linéaire)\n",
    "* `nn.Dropout` : applique un dropout à la couche, pour éviter le surapprentissage\n",
    "* `nn.ReLU` : fonction d'activation relu au coeur du réseau\n",
    "* `nn.Softmax` : en sortie de réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A COMPLETER\n",
    "\n",
    "# Créer le réseau suivant avec Pytorch :\n",
    "# 784 dimensions en entrée => 12 dense => activation relu => 12 dense => activation relu => dropout 0.5 => 10 sorties (activation softmax)\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # à compléter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # à compléter\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instanciation du modèle\n",
    "model = MyModel()\n",
    "\n",
    "# Déplace le modèle sur le device\n",
    "model.to(device)\n",
    "\n",
    "# affichage du résumé du modèle\n",
    "summary(\n",
    "    model, input_size=(1, 1, 28, 28)\n",
    ")  # input_size= (batch_size, channels, dim_x, dim_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " On commence par créer une boucle d'entrainement qui va faciliter l'apprentissage de notre modèle sur nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Défintion de la boucle d'entrainement\n",
    "def train_loop(\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: optim.SGD,\n",
    "    epoch: int,\n",
    ") -> torch.nn.Module:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Apprentissage du modèle\n",
    "    model.train()  # Met le modèle en mode entraînement\n",
    "    train_loss, count = 0.0, 0\n",
    "    for X, y in tqdm(train_dataloader, desc=f\"Train epoch {epoch}\"):\n",
    "        # Déplace les tenseurs sur le device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Prédiction du réseau de neurones\n",
    "        y_hat = model(X)\n",
    "        # Calcul de l'erreur (y_hat, y)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        # Backpropagation (MaJ des poids du réseau)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        count += X.shape[0]\n",
    "    train_loss /= count\n",
    "\n",
    "    # Test du modèle\n",
    "    accuracy = Accuracy(task=\"multiclass\", num_classes=10).to(\n",
    "        device\n",
    "    )  # Définition de la métrique d'accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss, count = 0.0, 0\n",
    "        for X, y in tqdm(test_dataloader, desc=f\"Test epoch {epoch}\"):\n",
    "            # Déplace les tenseurs sur le device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Prédiction du réseau de neurones\n",
    "            y_hat = model(X)\n",
    "            # Calcul de l'erreur (y_hat, y)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            accuracy.update(y_hat.argmax(dim=-1), y)\n",
    "            count += X.shape[0]\n",
    "\n",
    "    test_loss /= count\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\n",
    "        f\"\"\"\n",
    "        --------------- Epoch {epoch} --------------- \n",
    "        Training Loss : {train_loss:.4f} | Test Loss : {test_loss:.4f} | Test Accuracy : {accuracy.compute():.2f} | Elapsed Time : {round(elapsed_time, 1)}s\n",
    "        \"\"\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit également les données nécessaires à l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous pouvons lancer l'apprentissage des paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "sgd_optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\n",
    "for epoch in range(epochs):\n",
    "    model = train_loop(\n",
    "        train_dataloader, test_dataloader, model, loss_fn, sgd_optimizer, epoch\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous vous laissons analyser les résultats. Ce réseau de neurones est-il performant ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A vous de jouer__ : essayez de créer un meilleur réseau de neurones, afin d'atteindre le meilleur résultat possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un meilleur réseau de neurones, et l'entraîner\n",
    "# Objectif : avoir le meilleur résultat possible\n",
    "\n",
    "# Nous ne donnons pas la correction. Il y a plusieurs réponses possibles.\n",
    "# Vous pouvez par exemple ajouter des couches, modifier le nombre de neurones par couche et jouer sur le dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "sgd_optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\n",
    "for epoch in range(epochs):\n",
    "    model = train_loop(\n",
    "        train_dataloader, test_dataloader, model, loss_fn, sgd_optimizer, epoch\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons ce que donne notre modèle sur un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(model, dataloader):\n",
    "    # Parcourez un batch dans le DataLoader\n",
    "    for images, labels in dataloader:\n",
    "        preds = model(images.to(device))\n",
    "        sample_images = images[:5]  # Les 5 premières images\n",
    "        sample_labels = labels[:5]  # Les 5 premières étiquettes\n",
    "        sample_preds = torch.argmax(preds[:5], dim=1)  # Les 5 premières predictions\n",
    "        break  # On prend seulement le premier batch\n",
    "\n",
    "    # Affichez les images et leurs étiquettes\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(\n",
    "            sample_images[i].squeeze(), cmap=\"gray\"\n",
    "        )  # .squeeze() pour retirer la dimension du canal\n",
    "        ax.set_title(\n",
    "            f\"Label: {sample_labels[i].item()}, Pred: {sample_preds[i].item()}\"\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_prediction(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN : réseaux de neurones convolutionnels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant implémenter un réseau de neurones convolutionnel.\n",
    "\n",
    "Pour cet exercice, vous allez avoir besoin des méthodes Pytorch suivantes, en plus de celles déjà vues précédemment :\n",
    "\n",
    "Construisons notre premier réseau de neurones.\n",
    "\n",
    "* __nn.Conv2d__ : on ajoute une couche de convolution\n",
    "* __nn.MaxPool2d__ : fonction de max pooling qui permet de réduire la dimension des images d'entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le réseau suivant avec Pytorch :\n",
    "# input_shape dimensions en entrée => 4 filtres de convolution 3x3 => activation relu =>\n",
    "#    => maxpooling 2x2 => dropout 0.25 => flatten => 10 dense => dropout 0.5 =>\n",
    "#    => activation relu => nb_classes sorties (activation softmax)\n",
    "\n",
    "# Définition du modèle\n",
    "class MyCNNModel(nn.Module):\n",
    "    def __init__(self, input_shape, nb_classes):\n",
    "        super().__init__()\n",
    "        # à compléter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # à compléter\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instanciation du modèle\n",
    "cnn_model = MyCNNModel()\n",
    "\n",
    "# Déplace le modèle sur le device\n",
    "cnn_model.to(device)\n",
    "\n",
    "# affichage du résumé di modèle\n",
    "summary(model, imput_size=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "sgd_optimizer = optim.SGD(cnn_model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\n",
    "for epoch in range(epochs):\n",
    "    cnn_model = train_loop(\n",
    "        train_dataloader, test_dataloader, cnn_model, loss_fn, sgd_optimizer, epoch\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(cnn_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un meilleur réseau de neurones convolutionnel, et l'entraîner\n",
    "# Objectif : avoir le meilleur résultat possible\n",
    "\n",
    "# Nous ne donnons pas la correction. Il y a plusieurs réponses possibles.\n",
    "# Vous pouvez par exemple ajouter des couches convolutionnelles et max_pooling,\n",
    "# modifier le nombre de convolutions ou leur taille et jouer sur le dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "sgd_optimizer = optim.SGD(cnn_model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\n",
    "for epoch in range(epochs):\n",
    "    cnn_model = train_loop(\n",
    "        train_dataloader, test_dataloader, cnn_model, loss_fn, sgd_optimizer, epoch\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
