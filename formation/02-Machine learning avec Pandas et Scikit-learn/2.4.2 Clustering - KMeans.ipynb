{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering: K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous présentons dans ce notebook l'algorithme de **clustering** dit des **K moyennes**( ou **K-Means**). C'est un **algorithme d'apprentissage non supervisé**.\n",
    "\n",
    "Commençons par importer certaines libraires qui nous seront utiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation du K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le K Means est un algorithme de classification  **non supervisée**: cela signifie que le modèle cherche à comprendre la structure générale des données sans chercher à prédire une variable spécifiquement. \n",
    "\n",
    "Le K Means est assez intuitif, il cherche simplement à définir des centres de points proches dans les données. Le **centre** d'un **segment (cluster)**, va être défini comme étant la moyenne des points qui constitue le cluster. Pour attribuer  un cluster à un point, on cherche le centre qui est le plus proche du points\n",
    "\n",
    "Réutilisons les données utilisées précédemment mais en supprimant les couleurs car nous ne disposons par de labelisation dans ce cas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par simple observation, on identifie facilement que ces données peuvent être segmentés en quatre sous-ensembles homogènes. Si on cherche à calculer l'ensemble des segmentations de points possibles, on a un quatité exponetielle du nombre de points à évaluer. Cependant il existe une méthode de *maximisation de vraisemblance* efficace pour déterminer la segmentation optimale de manière plus rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "est = KMeans(4)  # 4 clusters\n",
    "est.fit(X)\n",
    "y_kmeans = est.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme segmente les points en 4 clusters de manière similaire à celle qu'un humain ferait par l'observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'algorihtme du K Means: maximisation de la vraisemblance\n",
    "\n",
    "Le K-means utilise une méthode de maximisation de la vraisemblance pour dtéerminer une segmentation optimale. Cela consiste à:\n",
    "\n",
    "1.Générer aléatoirement de centres de clusters\n",
    "\n",
    "2.Répéter jusqu'à convergence:\n",
    "    \n",
    "    A.Regroupement des points au centre le plus proche\n",
    "    B.Mise à jour des centres aux moyennes des clusters \n",
    "\n",
    "L'algorithme converge quand les centres stables après mise à jour. Les points restent figé dans leur segment.\n",
    "\n",
    "Observons la convergence du K Means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fig_code import plot_kmeans_interactive\n",
    "plot_kmeans_interactive();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme converge (presque) systématiquement vers la solution optimale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation du K Means\n",
    "\n",
    "La converge n'est pas garantie. Pour cette raison, la libraire scikit-learn utilise par défaut de multiples valuers d'initialisations des centre and conserve uniquement la meilleure solution.\n",
    "\n",
    "De plus, le nombre de cluster K doit être fixé à priori. D'autres méthodes détermine une valuer optimale du nombre de clusters le plus adapté aux données.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple: clustering de chiffres manuscrits par le K Means\n",
    "\n",
    "On s'intéresse à nouveaux aux données de chiffres manuscrits pour donner un exemple conret. Ici, nous utilisons le K Means pour regrouper automatiquement toutes les images qui représente un même chiffre. On va s'intéresser aux valeurs des centres obtenus par le K Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = KMeans(n_clusters=10)\n",
    "clusters = est.fit_predict(digits.data)\n",
    "est.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons 10 clusters de vecteurs en dimensions 64. Visualisons ces 10 centres de clusters pour comprendre ce qu'ils contiennent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
    "    ax.imshow(est.cluster_centers_[i].reshape((8, 8)), cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sans disposer d'annotations à priori**, le K Means est en mesure de déterminer des cluters dont le centre correspondent visualement aux chiffres de 0 à 9 (sauf le 8!).\n",
    "\n",
    "Les clusters ne sont par contre pas dans le bon ordre, on se propose de corriger cela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(digits.target[mask])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vérifier la pertinence de note clustering, évaluons la perfomrance de l'algorithme en classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(digits.target, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80%, c'est un score honorable! La matrice de confusion nous permet d'évaluer plus en détails ces performances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(digits.target, labels))\n",
    "\n",
    "plt.imshow(confusion_matrix(digits.target, labels),\n",
    "           cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.ylabel('true')\n",
    "plt.xlabel('predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour rappel, ce score de 80% précison en classification est obtenue **sans aucune supervision** de l'algorihtme, c'est à dire **aucune annotation préalable des données**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autre exemple: KMeans pour la compression de couleurs\n",
    "\n",
    "\n",
    "La compression de couleur consitute une application intéressante du clustering. Imaginons que l'on dispose d'une image avec potentiellment des millions de couleurs. Une majorité de ces couleurs va certainement être inutilisée par une image donnée. Et réciproquement, une même couleur peut être potentiellement utilisés par de nombreux pixels de l'image.\n",
    "\n",
    "Scikit-learn propose des images à manipuler dans le module ``datasets`` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "china = load_sample_image(\"china.jpg\")\n",
    "plt.imshow(china)\n",
    "plt.grid(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette image est stockée dans un array à 3 dimensions, de taille ``(height, width, RGB)``:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette image peut-être vue comme un suage de points dans une espace de couleurs à 3 dimensions. On normalise les couleurs pour qu'elle soit comprise entre 0 et 1 :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (china / 255.0).reshape(-1, 3)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dipose de 273 280 points dans un espace de 3 dimensions.\n",
    "\n",
    "La tâche est de compresser ces $256^3$ couleurs dans un nombre significativement plus petit, à savoir 64 couleurs. Concrètement, on cherche à obtenir 64 clusters dans nos données et recréer par la suite une image similaire en replacant chaque couleur (ou point) par le centre de son cluster le plus proche.\n",
    "\n",
    "Dans ce cas, on utilise le module ``MiniBatchKMeans``, une version plus sophistiquée qui permet de traiter des volumes de données plus important dans un délais raccourci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut réduire la taille de l'image pour accélerer la convergence\n",
    "n_colors = 64\n",
    "\n",
    "X = (china / 255.0).reshape(-1, 3)\n",
    "    \n",
    "model = MiniBatchKMeans(n_colors)\n",
    "labels = model.fit_predict(X)\n",
    "colors = model.cluster_centers_\n",
    "new_image = colors[labels].reshape(china.shape)\n",
    "new_image = (255 * new_image).astype(np.uint8)\n",
    "\n",
    "# La nouvelle image est créee puis affichée\n",
    "with plt.style.context('seaborn-white'):\n",
    "    plt.figure()\n",
    "    plt.imshow(china)\n",
    "    plt.title('input: 16 million colors')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(new_image)\n",
    "    plt.title('{0} colors'.format(n_colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On compare l'image d'origine avec l'image compressée. On a reduit le nombre de couleur, initialement égal à $256^3$, à uniquement 64 couleurs. Le résultat reste très satisfaisant !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
