{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage supervisé: Forêts d'arbres aléatoires (Random Forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intéressons nous maintenant à un des algorithmes les plus popualires de l'état de l'art. Cet algorithme est non-paramétrique et porte le nom de **forêts d'arbres aléatoires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A l'origine des forêts d'arbres aléatoires : l'arbre de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fôrets aléatoires appartiennent à la famille des méthodes d'**apprentissage ensembliste** et sont construits à partir d'**arbres de décision**. Pour cette raison, nous allons tout d'abord présenter les arbres de décisions.\n",
    "\n",
    "Un arbre de décision est une manière très intuitive de résoudre un problème de classification. On se contente de définir un certain nombre de questions qui vont permetre d'identifier la classe adéquate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fig_code.figures as fig\n",
    "fig.plot_example_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le découpage binaire des données est rapide a mettre en oeuvre. La difficulté va résider dans la manière de déterminer quelle est la \"bonne\" question à poser.\n",
    "\n",
    "C'est tout l'enjeu de la phase d'apprentissage d'un arbre de décision. L'algorithme va déterminer, au vue d'un ensemble de données, quelle question (ou découpage...) va apporter le plus gros gain d'information.\n",
    "\n",
    "### Construction d'un arbre de décision\n",
    "Voici un exemple de classifieur à partir d'un arbre de décision en utlisiant la libraire scikit-learn.\n",
    "\n",
    "Nous commencons par définir un jeu de données en 2 dimensions avec des labels associés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons précemment défini une fonction qui va faciliter la visualisation du processus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fig_code.figures import visualize_tree, plot_tree_interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise maintenant le module ``interact`` dans Ipython pour visualiser les découpages effectués par l'arbre de décision en fonction de la profondeur de l'arbre (*depth* en anglais), i.e. le nombre de questions que l'arbre peut poser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree_interactive(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque** : à chaque augmentation de la profondeur de l'arbre, chaque branche est découpée en deux **à l'expection** des branches qui contiennent uniquement des points d'une unique classe.\n",
    "\n",
    "L'arbre de décision est une méthode de classification non paramétrique facile à mettre en oeuvre\n",
    "\n",
    "**Question: Observez-vous des problèmes avec cette modélisation ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbre de décision et sur-apprentissage\n",
    "\n",
    "Un problème avec les arbres de décision est qu'ils ont tendance à **sur-apprendre** rapidement sur les données d'apprentissage. En effet, ils ont une forte tendance à capturer le bruit présent dans les données plutôt que la vraie distribution recherchée. Par exemple, si on construit 2 arbres à partir de sous ensembles des données définies précédemment, on obtient les deux classifieurs suivants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "plt.figure()\n",
    "visualize_tree(clf, X[:200], y[:200], boundaries=False)\n",
    "plt.figure()\n",
    "visualize_tree(clf, X[-200:], y[-200:], boundaries=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 2 classifieurs ont des différences notables si on regarde en détails les figures. Lorsque'on va prédire la classe d'un nouveau point, cela risque d'être impacté par le bruit dans les données plus que par le signal que l'on cherche à modéliser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédictions ensemblistes: Forêts aléatoires\n",
    "Une façon de limiter ce problème de sur-apprentissage est d'utiliser un **modèle ensembliste**: un méta-estimateur qui va aggréger les predictions de mutliples estimateurs (qui peuvent sur-apprendre individuellement). Grace à des propriétés mathématiques plutôt magiques (!), la prédiction aggrégée de ces estimateurs s'avère plus performante et robuste que les performances des estimateurs considérés individuellement.\n",
    "\n",
    "Une des méthodes ensemblistes les plus célèbres est la méthode des **forêts d'arbres aléatoires** qui aggrège les prédictions de multiples arbres de décision.\n",
    "\n",
    "Il y a beaucoup de littératures scientifiques pour déterminer la façon de rendre aléatoires ces arbres mais donner un exemple concret, voici un ensemble de modèle qui utilise seulement un sous échantillon des données :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                      random_state=0, cluster_std=2.0)\n",
    "\n",
    "def fit_randomized_tree(random_state=0):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    i = np.arange(len(y))\n",
    "    rng.shuffle(i)\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=5)\n",
    "    #on utilise seulement 250 exemples choisis aléatoirement sur les 300 disponibles\n",
    "    visualize_tree(clf, X[i[:250]], y[i[:250]], boundaries=False,\n",
    "                   xlim=(X[:, 0].min(), X[:, 0].max()),\n",
    "                   ylim=(X[:, 1].min(), X[:, 1].max()))\n",
    "    \n",
    "from ipywidgets import interact\n",
    "interact(fit_randomized_tree, random_state=(0, 100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut observer dans le détail les changements du modèle en fonction du tirage aléatoire des données qu'il utilise pour l'apprentissage, alors que la distribution des données est figée !\n",
    "\n",
    "La forêt aléatoire va faire des caluls similaires, mais va aggréger l'ensemble des arbres aléatoires générés pour construire une unique prédiction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "visualize_tree(clf, X, y, boundaries=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, y)\n",
    "visualize_tree(clf,X, y, boundaries=False)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=200, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En moyennant 100 arbres de décision \"perturbés\" aléatoirement, nous obtenons une prédiction aggrégé qui modélise avec plus de précision nos données.\n",
    "\n",
    "*(Remarque: ci dessus, notre perturbation aléatoire est effectué en echantillonant de manière aléatoire nos données... Les arbres aléatoires utilisent des techniques plus sophistiquées, pour plus de détails voir la [documentation de scikit-learn](http://scikit-learn.org/stable/modules/ensemble.html#forest)*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple 1 : utilisation en régression\n",
    "On considère pour cet exemple un cas d'tétude différent des exemples précédent de classification. Les arbres aléatoires peuvent être également utilisés sur des problèmes de régression (c'est à dire la prédiction d'une variable continue plutôt que discrète).\n",
    "\n",
    "L'estimateur que nous utiliserons est le suivant: ``sklearn.ensemble.RandomForestRegressor``.\n",
    "\n",
    "Nous présentons rapidement comment il peut être utilisé:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# On commence par créer un jeu de données d'apprentissage\n",
    "x = 10 * np.random.rand(100)\n",
    "\n",
    "def model(x, sigma=0.):\n",
    "    # sigma controle le bruit \n",
    "    # sigma=0 pour avoir une distribution \"parfaite\"\n",
    "    oscillation_rapide = np.sin(5 * x)\n",
    "    oscillation_lente = np.sin(0.5 * x)\n",
    "    bruit = sigma * np.random.randn(len(x))\n",
    "\n",
    "    return oscillation_rapide + oscillation_lente + bruit\n",
    "\n",
    "y = model(x)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(0, 10, num=1000)\n",
    "# yfit contient les prédictions de la forêt aléatoire à partir des données bruités\n",
    "yfit = RandomForestRegressor(100).fit(x[:, None], y).predict(xfit[:, None])\n",
    "# ytrue contient les valuers du modèle qui génèrent nos données avec un bruit nul \n",
    "ytrue = model(xfit, sigma=0) \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "#plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit, '-r', label = 'forêt aléatoire')\n",
    "plt.plot(xfit, ytrue, '-g', alpha=0.5, label = 'distribution non bruitée')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que les forêts aléatoires, de manière non-paramétrique, arrivent à estimer une distribution avec de mutliples périodicités sans aucune intervention de notre part pour définir ces périodicités !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Hyperparamètres**\n",
    "\n",
    "Utilisons l'outil d'aide inclus dans Ipython pour explorer la classe ``RandomForestRegressor``. Pour cela on rajoute un ? à la fin de l'objet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRegressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle sont les options disponibles  pour le ``RandomForestRegressor``?\n",
    "Quelle influence sur le graphique précédent si on modifie ces valeurs?\n",
    "\n",
    "Ces paramètres de classe sont appelés les **hyperparamètres** d'un modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice : proposer un modèle de régression à vecteur support permettant de modéliser le phénomène\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "SVMreg = SVR().fit(x[:, None], y)\n",
    "\n",
    "yfit_SVM = SVMreg.predict(xfit[:, None])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit_SVM, '-r', label = 'SVM')\n",
    "plt.plot(xfit, ytrue, '-g', alpha=0.5, label = 'distribution non bruitée')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
