{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principes de base de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons tout d'abord présenter la classe object *Estimator* de scikit-learn avant de revenir sur la notion d'**apprentissage supervisé**, sur des problèmes de *classification* et de *régression*, ainsi que l'**apprentissage\n",
    "non supervisé** sur des problèmes de *clustering* et *reduction de dimensions*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'objet `Estimator` de `Scikit-Learn`\n",
    "\n",
    "Tout algorithme présent dans `Scikit-Learn` hérite de la classe objet `Estimator`. C'est le cas par exemple de la régression linéaire:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paramètres de l'estimateur**: tous les paramètres de l'estimateur sont initialisés (en général sur des valeurs par défaut pertinentes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(positive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paramètre du modèle à estimer**: Quand un modèle est ajusté sur des données, les paramètres du modèle sont stockées dans les attributs de l'`Estimator` qui se termine par un `_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "y = 2 * x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données en 2D: ( 10 exemples et une seule features X)\n",
    "X = x[:, np.newaxis]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ajuste le modèle à partir des données \n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les paramètres appris se terminent par un underscore\n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modèle de regression linéaire à une pente (`coef_`) égale à 2 et l'origine (`intercept_`) vaut 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage supervisé: Classification and Régression\n",
    "\n",
    "En **apprentissage supervisé**, nous diposons d'un jeu de données constitué de prédicteurs (features) couplées à des annotations (labels). La tâche consiste à contruire un estimateur qui va prédire l'annotations en utilisant uniquement des prédicteurs. Pour donner un exemple simple: on cherche à prédire l'espèce d'une iris (une plante) en considérant uniquement les dimensions mesurées de ses feuilles.\n",
    "D'autres exmples plus difficles à résoudre:\n",
    "- indentifier de la neige sur une photo de webcam.\n",
    "- déterminer la température qui va etre observée sur une station à partir d'un ensemble de température prévues par différents modèles de PNT (AS Mixtes).\n",
    "- recommander des films d'intérêts à l'utilisateur à partir d'un historique de films vus par un utilisateur et de notes attribuées. Le système de recommandation: un exemple connu est celui du [Challenge Netflix](http://en.wikipedia.org/wiki/Netflix_prize)\n",
    "\n",
    "\n",
    "Le point commun entre ces exemples est que l'on cherche toujours à prédire une (ou plusieurs) variable(s) à partir d'autres variables connues.\n",
    "\n",
    "L'apprentissage supervisé se divise en 2 grandes sous-catégories: la **classification** et la **régression**, selon la nature de la variable à prédire. Si la variable est discrète, on parle de classification (ex: neige / pas de neige). Si la variable est continue, on parle de régression (ex: température)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de classification\n",
    "La **méthode des K plus proches voisins** (K Nearest Neighbors, souvent abregé KNN) figure parmis les méthodes les plus simples: l'algorithme va cherches les exemples similaires dans nos données et prédit la classe dominante dans son voisinage.\n",
    "\n",
    "Regardons ce que cela donne en pratique sur les **données d'Iris** qui contiennent 4 variables descriptives :\n",
    "- longueurs des sépales,\n",
    "- largeurs des sépales,\n",
    "- longueurs des pétales,\n",
    "- et largeur des pétales;\n",
    "\n",
    "pour sur 3 types d'iris distinct:\n",
    "- setosa (rouge)\n",
    "- versicolor (vert)\n",
    "- virginia (bleu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Iris_dataset_scatterplot.svg.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Définition du modèle\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Apprentissage\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Quel type d'iris pour une sépale de 3cm x 5cm et pétale de 4cm x 2cm?\n",
    "# on utilise la méthode .predict():\n",
    "result = knn.predict([[3, 5, 4, 2],])\n",
    "\n",
    "print(iris.target_names[result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut accéder aux prédictions sous forme de probabilités d'appartenance aux classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba([[3, 5, 4, 2],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fig_code import plot_iris_knn\n",
    "plot_iris_knn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de régression\n",
    "\n",
    "Une des approches les plus basique en régression va consister à apprendre une droite qui modélise au mieux les données comme nous avons vu au début de ce notebook.\n",
    "\n",
    "Scikit learn propose également des algortihmes de régressions plus sophistiqués."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On génère un jeu de données simple\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = np.random.random(size=(20, 1))\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(20)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précédemment, on estime la droite optimale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# On trace les données et nos prédictions\n",
    "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_fit = model.predict(X_fit)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o')\n",
    "plt.plot(X_fit.squeeze(), y_fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple de modèle plus sophistiqué avec une forêt aléatoire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apprentissage d'une forêt aléatoire\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot the data and the model prediction\n",
    "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_fit = model.predict(X_fit)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o')\n",
    "plt.plot(X_fit.squeeze(), y_fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Déterminer si ces modélisations sont bonnes va dépendre de plusieurs facteurs.\n",
    "\n",
    "Nous aborderons par la suite la méthodologie pour déterminer la meilleure approche pour un problème donné.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage non-supervisé: Réduction de dimension et clustering\n",
    "\n",
    "L'**apprentissage non supervisé** résout un autre type de problème. Ici, nous ne disposons pas de variable(s) cible(s) à modéliser, mais uniquement à établir des regroupements dans nos données.\n",
    "On peut voir l'apprentissage non supervisé comme une manière de définir des classes mais à la différence de la classification évoqué plus haut, elles ne sont pas établies à priori. \n",
    "De plus, sur les données d'iris vues précedemment, on peut rechercher des combinaisons des mesures des sépales et pétales qui résument au mieux les caractistiques des Iris. Nous verrons que l'on peut projeter nos 4 variables descriptives dans un espace en 2 dimensions de manière pertinentes, ce qui permet par la suite une visualisation dans un unique graphique.\n",
    "\n",
    "Quelques exemples de problèmes non supervisés:\n",
    "- à partir d'observations satellites en grande dimensions de galaxies lointaines, chercher quelques variables ou quelques combinaisons de variables résument au mieux l'information contenue dans ces données.\n",
    "- à partir d'un mélange de sources audios, par exemple la voix d'une personne avec de la musique en fond sonore, isoler les deux sources audios.\n",
    "- déterminer un régime de temps sur l'Atlantique Nord à partir de quelques champs météorologiques.\n",
    "\n",
    "L'apprentissage non supérvisé peut également être combiné à de l'apprentissage supervisé, c'est le cas en grande dimension ou en présence de sources de données hétérogènes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction de dimension: ACP\n",
    "\n",
    "L'**Analyse en Composantes Principales** (ACP)  est une technique de réduction de dimensions qui détermine les projections des différentes variables qui explique la plus grandes proportions de variance dans les données.\n",
    "\n",
    "Si on conserve l'exemple des données d'iris. On ne peut visualiser en 2 dimensions des observations décrites par 4 variables. On va extraire 2 projections de ces 4 variables pour obtenir une représenattion bidimensionnelle pour la visualiser sur une figure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris.data, iris.target\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "X_reduced = pca.transform(X)\n",
    "print(\"Dimension des données réduites:\", X_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y,\n",
    "           cmap='RdYlBu')\n",
    "\n",
    "print(\"Détails des 2 composantes:\")\n",
    "for component in pca.components_:\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name)\n",
    "                     for value, name in zip(component,\n",
    "                                            iris.feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering: K-means\n",
    "Le clustering a pour but de créer des sous-ensembles d'observations homogènes, appelés **clusters**. Pour déterminer la proximité entre exemples, on doit définir une métrique. \n",
    "\n",
    "Le choix de cette métrique est déterminant pour la découverte d'une structure pertinente dans les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=3, random_state=0) # Fixing the RNG in kmeans\n",
    "k_means.fit(X)\n",
    "y_pred = k_means.predict(X)\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_pred,\n",
    "           cmap='RdYlBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recapitulatifs: `Estimator`\n",
    "\n",
    "`Scikit-Learn` propose une méta-classe unique pour l'ensemble des méthodes d'apprentissages. Pour un objet `model` donné hérité de cette classe, les options suivantes sont disponibles:\n",
    "\n",
    "- **Disponible sur tout `Estimator`**\n",
    "  + `model.fit()` : Apprentissage sur les données. Pour l'apprentissage supervisé,\n",
    "    on fournit 2 paramètres `X` et `y` : les variables explicatives `X` et la variables cible `y` (i.e. `model.fit(X, y)`).\n",
    "    Pour l'apprentissage non supervisé, un seul paramètre `X` suffit (i.e. `model.fit(X)`)\n",
    "- **Disponible sur `Estimator` d'apprentissage supervisé**\n",
    "  + `model.predict()` : pour un modèle déjà appris, on l'applique sur des nouvelles variables descriptives `X_new` (i.e. `model.predict(X_new)`) et on crée une prédiction pour la cible de chaque observation.\n",
    "  + `model.predict_proba()` : Dans le cas de la classification, on dispose pour certains modèles de la probabilité d'appartenance à une classe. Uniquement la classe associé à la probabilité la plus élevée est disponible en utilisant `model.predict()`.\n",
    "- **Disponibles sur `Estimator` d'apprentissage non supervisé**\n",
    "  + `model.predict()` : prédit les clusters des nouvelles observations.\n",
    "  + `model.transform()` : pour un apprentissage non supervisé, transforme de nouvelles données dans le nouvel espace appris par le modèle non supervisé.\n",
    "  + `model.fit_transform()` : certains estimateurs dispose de cette méthode qui utilise une unique commande pour apprendre une structure non-spervisé sur les données et appliquer directement cette transformation à la volée (c'est le cas pour l'ACP par exemple)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
