{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principes de base de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons tout d'abord présenter la classe object *Estimator* de scikit-learn avant de revenir sur la notion d'**apprentissage supervisé**, sur des problèmes de *classification* et de *régression*, ainsi que l'**apprentissage\n",
    "non supervisé** sur des problèmes de *clustering* et *reduction de dimensions*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'objet Estimator de Scikit-Learn\n",
    "\n",
    "Tout algorithme présent dans Scikit-learn hérite de la classe objet ''Estimator''. C'est le cas par exemple de la régression linéaire:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paramètres de l'estimateur**: tous les paramètres de l'estimateur sont initialisés (en général sur des valeurs par défaut pertinentes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(positive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paramètre du modèle à estimer**: Quand un modèle est appris sur des données, la valeur des paramètres du modèle est stockée dans des attributs de l'object estimateur qui se termine par un underscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "y = 2 * x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données en 2D: ( 10 exemples et une seule features X)\n",
    "X = x[:, np.newaxis]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On apprend le model à partir des données \n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les paramètres appris se terminent par un underscore\n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modèle de regression linéaire à une pente égale à 2 et l'origine vaut 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage supervisé: Classification and Régression\n",
    "\n",
    "En **apprentissage supervisé**, nous diposons d'un jeu de données constitué de descripteurs (features) couplées à des annotations (labels). La tâche consiste à contruire un estimateur qui va prédire l'annotations en utilisant uniquement des descripteurs. Pour donner un exemple simple: on cherche à prédire l'espèce d'une iris (une plante) en considérant uniquement les dimensions mesurées de ses feuilles.\n",
    "D'autres exmples plus difficles à résoudre:\n",
    "- à partir d'une image capturé via un téléscope, déterminer si elle contient une étoile, un rayonnement stellaire ou une galaxie.\n",
    "- indentifier une personne sur une photo.\n",
    "- à partir d'un historique de films vus par un utilisateur et de notes attribuées, recommander des films d'intérêts à l'utilisateur. (Le système de recommandation: un exemple connu est celui du [Challenge Netflix](http://en.wikipedia.org/wiki/Netflix_prize))\n",
    "\n",
    "\n",
    "Le point commun entre ces exemples est que l'on cherche toujours à prédire une variable (ou plusieurs) à partir d'autres variables observées.\n",
    "\n",
    "L'apprentissage supervisé se divise en 2 grandes sous-catégories: la **classification** et la **régression**, selon la nature de la variable à prédire. Si la variable est discrète, on parle de classification. Si la variable est continue, on parle de régression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de classification\n",
    "La **méthode des K plus proches voisins** est une des méthodes les plus simple: pour un nouvel exemple, on cherche les exemples similaires dans nos données et on prédit la classe dominante dans son voisinage.\n",
    "\n",
    "Regardons ce que cela donne en pratique sur les **données d'iris**. Les données d'Iris contiennent 4 variables descriptives :\n",
    "- longueurs des sépales,\n",
    "- largeurs des sépales,\n",
    "- longueurs des pétales,\n",
    "- et largeur des pétales;\n",
    "\n",
    "pour sur 3 types d'iris distinct:\n",
    "- setosa (rouge)\n",
    "- versicolor (vert)\n",
    "- virginia (bleu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Iris_dataset_scatterplot.svg.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# définition du modèle\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# apprentissage\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Quel type d'iris pour une sépale de 3cm x 5cm et pétale de 4cm x 2cm?\n",
    "# on utilise la méthode .predict():\n",
    "result = knn.predict([[3, 5, 4, 2],])\n",
    "\n",
    "print(iris.target_names[result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut accéder aux prédictions sous forme de probabilités d'appartenance aux classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba([[3, 5, 4, 2],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fig_code import plot_iris_knn\n",
    "plot_iris_knn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de régression\n",
    "\n",
    "Une des approches les plus basique en régression va consister à apprendre une droite qui modélise au mieux les données comme nous avons vu au début de ce notebook.\n",
    "\n",
    "Scikit learn propose également des algortihmes de régrssions plus sophistiqués."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On génère un jeu de données simple\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = np.random.random(size=(20, 1))\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(20)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précédemment, on estime la droite optimale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# On trace les données et nos prédictions\n",
    "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_fit = model.predict(X_fit)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o')\n",
    "plt.plot(X_fit.squeeze(), y_fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple de modèle plus sophistiqué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apprentissage d'une forêt aléatoire\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot the data and the model prediction\n",
    "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_fit = model.predict(X_fit)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o')\n",
    "plt.plot(X_fit.squeeze(), y_fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Déterminer si ces modélisations sont bonnes va dépendre de plusieurs facteurs; nous aborderons par la suite la méthodolgie pour déterminer la meilleure approche pour un problème donné.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage non-supervisé: Réduction de dimension et clustering\n",
    "\n",
    "L'**apprentissage non supervisé** résout un autre type de problème. Ici , nous ne disposons pas de variable cible à modéliser. Nous cherchons uniquement à établir des regroupement d'observations similaires dans nos données.\n",
    "On peut voir l'apprentissage non supervisé comme une manière de définir des classes mais à la différence de la classification évoqué plus haut, elles sont pas établies à priori. \n",
    "Egalement, sur les données d'iris vues précedemment, on peut rechercher des combinaisons des mesures des sépales et pétales qui résument au mieux les caractistiques des Iris. Nous verrons que l'on peut projecter nos 4 variables descriptives dans un espace en 2D de manière pertinentes, ce qui permet par la suite une visualisation dans un unique graphique.\n",
    "\n",
    "Quelques exemples de problèmes non supervisés:\n",
    "- à partir d'observations satellites en grande dimensions de galaxies lointaines, chercher quelques variables ou quelques combinaisons de variables résument au mieux l'information contenue dans ces données.\n",
    "- à partir d'un mélange de sources audios, par exemple la voix d'une personne avec de la musique en fond sonore, isoler les deux sources audios.\n",
    "- à partir d'une video, isoler les objets en mouvement, et categoriser les différents objets détectés.\n",
    "\n",
    "L'apprentissage non supérvisé peut également être combiné à de l'apprentissage supervisé, c'est le cas en grande dimension ou en présence de sources de données hétérogènes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction de dimension: ACP\n",
    "\n",
    "L'analyse en composantes principales (ACP)  est une technique de réduction de dimensions qui détermine les projections des différentes variables qui explique la plus grandes proportions de variance dans les données.\n",
    "\n",
    "Si on conserve l'exemple des données d'iris. On ne peut visualiser en 2D des observations qui ont 4 variables. On va extraires 2 projections de ces 4 variables pour obtenir une représenattion 2D facile à visualiser sur une figure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris.data, iris.target\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "X_reduced = pca.transform(X)\n",
    "print(\"Dimension des données réduites:\", X_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y,\n",
    "           cmap='RdYlBu')\n",
    "\n",
    "print(\"Détails des 2 composantes:\")\n",
    "for component in pca.components_:\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name)\n",
    "                     for value, name in zip(component,\n",
    "                                            iris.feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering: K-means\n",
    "Le clustering a pour but de créer des sous-ensembles d'observations homogènes, appelés ''clusters''. Pour déterminer la proximité entre exemples, on doit définir une métrique. \n",
    "\n",
    "Le choix de cette métrique est déterminant pour la découverte d'une structure pertinente dans les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=3, random_state=0) # Fixing the RNG in kmeans\n",
    "k_means.fit(X)\n",
    "y_pred = k_means.predict(X)\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_pred,\n",
    "           cmap='RdYlBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recapitulatifs: Estimator\n",
    "\n",
    "Scikit learn propose une méta-classe unique pour l'ensemble des méthodes d'apprentissages. Pour un objet `model` donné hérité de cette classe, les options suivantes sont disponibles:\n",
    "\n",
    "- Disponible sur **tout Estimator**\n",
    "  + `model.fit()` : Apprentissage sur les données. Pour l'apprentissage supervisé,\n",
    "    on fournit 2 paramètres `X` et `y` : les variables explicatives `X` et la variables cible `y` (i.e. `model.fit(X, y)`).\n",
    "    Pour l'apprentissage non supervisé, un seul paramètre `X` suffit (i.e. `model.fit(X)`)\n",
    "- Disponible sur **Estimator d'apprentissage supervisé**\n",
    "  + `model.predict()` : pour un modèle déjà appris, on l'applique sur des nouvelles variables descriptives `X_new` (i.e. `model.predict(X_new)`) et on crée une prédiction pour la cible de chaque observation.\n",
    "  + `model.predict_proba()` : Dans le cas de la classification, on dispose pour certains modèles de la probabilité d'appartenance à une classe. Uniquement la classe associé à la probabilité la plus élevée est disponible en utilisant `model.predict()`.\n",
    "- Disponibles sur **Estimator d'apprentissage non supervisé**\n",
    "  + `model.predict()` : prédit les clusters des nouvelles observations.\n",
    "  + `model.transform()` : pour un apprentissage non supervisé, transforme de nouvelles données dans le nouvel espace appris par le modèle non supervisé.\n",
    "  + `model.fit_transform()` : certains estimateurs dispose de cette méthode qui utilise une unique commande pour apprendre une structure non-spervisé sur les données et appliquer directement cette transformation à la volée (c'est le cas pour l'ACP par exemple)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
